{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from bs4.element import Comment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self, df, stop_words):\n",
    "        self.df = df\n",
    "        self.train_cv = None\n",
    "        self.train_count_matrix = None\n",
    "        self.train_tfidf_matrix = None\n",
    "        self.nb_model = None\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "\n",
    "    def generate_train_test(self, train_size=0.75):\n",
    "        self.y = self.df.pop('fraud').values\n",
    "        self.X = self.df['desc_text'].values\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                                                                self.X, self.y, \n",
    "                                                                stratify=self.y, \n",
    "                                                                train_size=train_size,\n",
    "                                                                random_state = 123)\n",
    "    def join_list_of_strings(self, lst):\n",
    "        \"\"\"\n",
    "        Joins the list into a string\n",
    "        \n",
    "        Params\n",
    "        lst: list of words\n",
    "        \"\"\"\n",
    "        return [\" \".join(x) for x in lst]\n",
    "\n",
    "    def remove_X_train_stops(self):\n",
    "        lower = [x.lower() for x in self.X_train]\n",
    "        split_lst = [txt.split() for txt in lower]\n",
    "        self.stops_removed_lst = []\n",
    "\n",
    "        for split in split_lst:\n",
    "            stops = [w for w in split if w not in self.stop_words]\n",
    "            stop_num = [w for w in stops if not (w.isdigit() \n",
    "                        or w[0] == '-' and w[1:].isdigit())]\n",
    "            self.stops_removed_lst.append(stop_num)\n",
    "        self.X_train = self.join_list_of_strings(self.stops_removed_lst)\n",
    "\n",
    "    def remove_X_test_stops(self):\n",
    "        lower = [x.lower() for x in self.X_test]\n",
    "        split_lst = [txt.split() for txt in lower]\n",
    "        self.stops_removed_lst = []\n",
    "\n",
    "        for split in split_lst:\n",
    "            stops = [w for w in split if w not in self.stop_words]\n",
    "            stop_num = [w for w in stops if not (w.isdigit() \n",
    "                        or w[0] == '-' and w[1:].isdigit())]\n",
    "            self.stops_removed_lst.append(stop_num)\n",
    "        self.X_test = self.join_list_of_strings(self.stops_removed_lst)\n",
    "\n",
    "    def tf_idf_matrix(self):\n",
    "            \"\"\"\n",
    "            Sets up a word count matrix, a tfidf matrix, and a CountVectorizer for\n",
    "            the documents in the directory\n",
    "\n",
    "            Params\n",
    "            documents: list of strings to be vectorized\n",
    "\n",
    "            Returns\n",
    "            count_matrix: matrix with word counts\n",
    "            tfidf_matrix: a tfidf matrix of the documents\n",
    "            cv: CountVectorizer object for the documents\n",
    "            \"\"\"\n",
    "            self.train_cv = CountVectorizer()\n",
    "            self.train_count_matrix = self.train_cv.fit_transform(self.X_train)\n",
    "            tfidf_transformer = TfidfTransformer()\n",
    "            self.train_tfidf_matrix = tfidf_transformer.fit_transform(self.train_count_matrix)\n",
    "\n",
    "    def naive_bayes_model(self):\n",
    "            \"\"\"\n",
    "            Sets up a naive bayes model for the documents in the directory\n",
    "\n",
    "            Params\n",
    "            directory: directory for the documents\n",
    "            stop_words: list of stop_words for word filtration\n",
    "            technique: technique: str choose from ['porter', 'snowball','wordnet']\n",
    "\n",
    "            Returns\n",
    "            nb_model: a naive bayes model for the documents in the directory\n",
    "            cv: CountVectorizer object for the documents\n",
    "            \"\"\"\n",
    "            self.nb_model = MultinomialNB()\n",
    "            self.nb_model.fit(self.train_tfidf_matrix, self.y_train)\n",
    "\n",
    "    def return_top_n_words(self, n=7):\n",
    "            \"\"\"\n",
    "            Prints out the top n words for each document in the categories for the \n",
    "            documents in the directory\n",
    "\n",
    "            Params\n",
    "            directory: directory for the documents\n",
    "            stop_words: list of stop_words for word filtration\n",
    "            documents: a list of the categories (folders) in the directory\n",
    "            technique: technique: str choose from ['porter', 'snowball','wordnet']\n",
    "\n",
    "            \"\"\"\n",
    "            feature_words = self.train_cv.get_feature_names()\n",
    "            categories = self.nb_model.classes_\n",
    "            self.top_words_dic = {}\n",
    "            for cat in range(len(categories)):\n",
    "                print(f\"\\n Target: {cat}, name: {categories[cat]}\")\n",
    "                log_prob = self.nb_model.feature_log_prob_[cat]\n",
    "                i_topn = np.argsort(log_prob)[::-1][:n]\n",
    "                features_topn = [feature_words[i] for i in i_topn]\n",
    "                self.top_words_dic[categories[cat]] = features_topn\n",
    "                print(f\"Top {n} tokens: \", features_topn)\n",
    "    \n",
    "    def get_accuracy_classification_report(self):\n",
    "        \"\"\"\n",
    "        Prints out and returns the accuracy score from the prediction vs the actuals\n",
    "        for the test set\n",
    "\n",
    "        Params\n",
    "        train_docs: list of strs used to train on\n",
    "        test_docs: list of strs used to test\n",
    "        test_targes: list of strs for the test target values\n",
    "\n",
    "        Returns\n",
    "        Accuracy score for the model\n",
    "        \"\"\"\n",
    "        self.nb_pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                            ('tfidf', TfidfTransformer()),\n",
    "                            ('model', MultinomialNB()),\n",
    "                            ])\n",
    "\n",
    "        self.nb_pipeline.fit(self.X_train, self.y_train)\n",
    "        self.predicted = self.nb_pipeline.predict(self.X_test)\n",
    "        self.accuracy = np.mean(self.predicted == self.y_test)\n",
    "        print(\"\\nThe accuracy on the test set is {0:0.3f}.\".format(self.accuracy))\n",
    "        self.class_report = classification_report(self.y_test, self.predicted, \n",
    "                                                  digits=3,output_dict=True)\n",
    "    def predic_probablility(self, X):\n",
    "        return self.nb_pipeline.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/data.json')\n",
    "descriptions = df.description\n",
    "df['fraud'] = np.where((df['acct_type'] == 'fraudster') | \n",
    "                    (df['acct_type'] == 'fraudster_event') |\n",
    "                    (df['acct_type'] == 'fraudster_att'), 1, 0)\n",
    "descriptions = df.description\n",
    "df['desc_text'] = 0\n",
    "for i in range(df.shape[0]):\n",
    "    df.iloc[i,45] = text_from_html(descriptions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = df[df['fraud'] == 1]\n",
    "df_nf = df[df['fraud'] == 0]\n",
    "df_nf_subset = df_nf.sample(n=df_fraud.shape[0], replace=False)\n",
    "df_combined = df_fraud.append(df_nf_subset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1293, 46)\n",
      "(13044, 46)\n",
      "(1293, 46)\n",
      "(2586, 46)\n"
     ]
    }
   ],
   "source": [
    "print(df_fraud.shape)\n",
    "print(df_nf.shape)\n",
    "print(df_nf_subset.shape)\n",
    "print(df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9969135802469136\n",
      "1.001031991744066\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "extra_stops = ['tickets', '00']\n",
    "stop_words.extend(extra_stops)\n",
    "\n",
    "nb = NaiveBayes(df_combined, stop_words)\n",
    "nb.generate_train_test()\n",
    "nb.remove_X_train_stops()\n",
    "nb.remove_X_test_stops()\n",
    "\n",
    "test_counts_fraud = [x for x in nb.y_test if x == 1]\n",
    "test_counts_notfraud = [x for x in nb.y_test if x == 0]\n",
    "train_counts_fraud = [x for x in nb.y_train if x == 1]\n",
    "train_counts_notfraud = [x for x in nb.y_train if x == 0]\n",
    "print(len(test_counts_fraud)/len(test_counts_notfraud))\n",
    "print(len(train_counts_fraud)/len(train_counts_notfraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Target: 0, name: 0\n",
      "Top 7 tokens:  ['event', 'business', '00', 'please', 'com', 'us', 'new']\n",
      "\n",
      " Target: 1, name: 1\n",
      "Top 7 tokens:  ['de', 'event', 'party', 'conference', 'et', 'live', 'get']\n",
      "\n",
      "The accuracy on the test set is 0.742.\n",
      "<class 'numpy.ndarray'>\n",
      "(14337, 2)\n"
     ]
    }
   ],
   "source": [
    "nb.tf_idf_matrix()\n",
    "nb.naive_bayes_model()\n",
    "nb.return_top_n_words()\n",
    "nb.get_accuracy_classification_report()\n",
    "probs = nb.predic_probablility(df['description'])\n",
    "print(type(probs))\n",
    "print(probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14337 entries, 0 to 14336\n",
      "Data columns (total 46 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   acct_type           14337 non-null  object \n",
      " 1   approx_payout_date  14337 non-null  int64  \n",
      " 2   body_length         14337 non-null  int64  \n",
      " 3   channels            14337 non-null  int64  \n",
      " 4   country             14256 non-null  object \n",
      " 5   currency            14337 non-null  object \n",
      " 6   delivery_method     14321 non-null  float64\n",
      " 7   description         14337 non-null  object \n",
      " 8   email_domain        14337 non-null  object \n",
      " 9   event_created       14337 non-null  int64  \n",
      " 10  event_end           14337 non-null  int64  \n",
      " 11  event_published     14238 non-null  float64\n",
      " 12  event_start         14337 non-null  int64  \n",
      " 13  fb_published        14337 non-null  int64  \n",
      " 14  gts                 14337 non-null  float64\n",
      " 15  has_analytics       14337 non-null  int64  \n",
      " 16  has_header          8928 non-null   float64\n",
      " 17  has_logo            14337 non-null  int64  \n",
      " 18  listed              14337 non-null  object \n",
      " 19  name                14337 non-null  object \n",
      " 20  name_length         14337 non-null  int64  \n",
      " 21  num_order           14337 non-null  int64  \n",
      " 22  num_payouts         14337 non-null  int64  \n",
      " 23  object_id           14337 non-null  int64  \n",
      " 24  org_desc            14337 non-null  object \n",
      " 25  org_facebook        14278 non-null  float64\n",
      " 26  org_name            14337 non-null  object \n",
      " 27  org_twitter         14278 non-null  float64\n",
      " 28  payee_name          14337 non-null  object \n",
      " 29  payout_type         14337 non-null  object \n",
      " 30  previous_payouts    14337 non-null  object \n",
      " 31  sale_duration       14182 non-null  float64\n",
      " 32  sale_duration2      14337 non-null  int64  \n",
      " 33  show_map            14337 non-null  int64  \n",
      " 34  ticket_types        14337 non-null  object \n",
      " 35  user_age            14337 non-null  int64  \n",
      " 36  user_created        14337 non-null  int64  \n",
      " 37  user_type           14337 non-null  int64  \n",
      " 38  venue_address       14337 non-null  object \n",
      " 39  venue_country       13261 non-null  object \n",
      " 40  venue_latitude      13261 non-null  float64\n",
      " 41  venue_longitude     13261 non-null  float64\n",
      " 42  venue_name          13261 non-null  object \n",
      " 43  venue_state         13261 non-null  object \n",
      " 44  fraud               14337 non-null  int64  \n",
      " 45  desc_text           14337 non-null  object \n",
      "dtypes: float64(9), int64(19), object(18)\n",
      "memory usage: 5.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57838133 0.42161867]\n",
      " [0.93469005 0.06530995]\n",
      " [0.77651694 0.22348306]\n",
      " [0.78722535 0.21277465]\n",
      " [0.89012939 0.10987061]\n",
      " [0.68672512 0.31327488]\n",
      " [0.49974214 0.50025786]\n",
      " [0.82177585 0.17822415]\n",
      " [0.58239931 0.41760069]\n",
      " [0.82177585 0.17822415]]\n"
     ]
    }
   ],
   "source": [
    "print(probs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-fb990b0f2f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_top_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-774d2419f523>\u001b[0m in \u001b[0;36mreturn_top_n_words\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mfeature_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_words_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "nb.return_top_n_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
